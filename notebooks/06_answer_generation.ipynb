{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "460f060d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Loaded Config. Using Model: llama-3.3-70b-versatile\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "import json\n",
    "from pathlib import Path\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# Load environment\n",
    "load_dotenv()\n",
    "project_root = Path.cwd().parent if 'notebooks' in str(Path.cwd()) else Path.cwd()\n",
    "sys.path.insert(0, str(project_root))\n",
    "\n",
    "# LLM Imports\n",
    "from langchain_groq import ChatGroq\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "# Config Loading\n",
    "with open(project_root / \"phase4_config.json\", 'r') as f: phase4_config = json.load(f)\n",
    "\n",
    "print(f\" Loaded Config. Using Model: {phase4_config['llm_model']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8f43a3a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Groq LLM Initialized\n"
     ]
    }
   ],
   "source": [
    "# Initialize Groq LLM\n",
    "llm = ChatGroq(\n",
    "    model_name=phase4_config['llm_model'],\n",
    "    temperature=0.3, # Low temperature for factual accuracy\n",
    "    api_key=os.getenv(\"GROQ_API_KEY\")\n",
    ")\n",
    "\n",
    "print(\"✅ Groq LLM Initialized\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "85cc7368",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prompt Template Ready\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Define Prompt Template\n",
    "\n",
    "system_template = \"\"\"\n",
    "You are an expert Agricultural Assistant helping Indian farmers. \n",
    "Your goal is to answer their questions accurately using ONLY the provided context.\n",
    "\n",
    "### GUIDELINES:\n",
    "1. **Tone**: Be empathetic, professional, and simple. Use clear English.\n",
    "2. **Structure**: \n",
    "   - Start with a direct answer.\n",
    "   - Use bullet points for steps or lists.\n",
    "   - If discussing a chemical, explicitly mention safety warnings if present in context.\n",
    "3. **Accuracy**: \n",
    "   - Answer ONLY based on the \"Context\" below. \n",
    "   - If the answer is not in the context, say: \"I am sorry, but I could not find that information in my reference documents.\"\n",
    "   - DO NOT make up information.\n",
    "4. **Citations**: \n",
    "   - At the end, list the specific Source Documents used (e.g., \"Source: CitrusPlantPestsAndDiseases.pdf\").\n",
    "\n",
    "### CONTEXT:\n",
    "{context}\n",
    "\n",
    "### CHAT HISTORY:\n",
    "{chat_history}\n",
    "\n",
    "### USER QUESTION:\n",
    "{question}\n",
    "\"\"\"\n",
    "\n",
    "prompt = ChatPromptTemplate.from_template(system_template)\n",
    "chain = prompt | llm | StrOutputParser()\n",
    "\n",
    "print(\"Prompt Template Ready\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6ad2b01f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "`format_context` function ready\n"
     ]
    }
   ],
   "source": [
    "# Function to format context for LLM\n",
    "\n",
    "def format_context(docs):\n",
    "    \"\"\"\n",
    "    Takes the raw list of dicts from Phase 5 and formats it into a string \n",
    "    that the LLM can read easily.\n",
    "    \"\"\"\n",
    "    formatted_text = \"\"\n",
    "    sources = set()\n",
    "    \n",
    "    for i, doc in enumerate(docs, 1):\n",
    "        content = doc.get('content', '').replace('\\n', ' ')\n",
    "        source = doc.get('metadata', {}).get('source', 'Unknown File')\n",
    "        \n",
    "        formatted_text += f\"[Document {i} - Source: {source}]\\n{content}\\n\\n\"\n",
    "        sources.add(source)\n",
    "        \n",
    "    return formatted_text, list(sources)\n",
    "\n",
    "print(\"`format_context` function ready\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f066c447",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "`generate_answer` function ready\n"
     ]
    }
   ],
   "source": [
    "# Function to generate answer using LLM\n",
    "\n",
    "def generate_answer(question: str, retrieved_docs: list, chat_history: str = \"\"):\n",
    "    \"\"\"\n",
    "    Combines Context + Prompt + LLM to get the final answer.\n",
    "    \"\"\"\n",
    "    # 1. Prepare Context\n",
    "    context_text, sources = format_context(retrieved_docs)\n",
    "    \n",
    "    if not context_text:\n",
    "        return \"I'm sorry, I couldn't find any relevant documents to answer your question.\"\n",
    "    \n",
    "    # 2. Invoke LLM\n",
    "    response = chain.invoke({\n",
    "        \"context\": context_text,\n",
    "        \"chat_history\": chat_history,\n",
    "        \"question\": question\n",
    "    })\n",
    "    \n",
    "    return response\n",
    "\n",
    "print(\"`generate_answer` function ready\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61d944a8",
   "metadata": {},
   "source": [
    "### Testing with Mock Data\n",
    "Before we connect the full retrieval pipeline, let's test if the LLM generates good answers using **fake** retrieved data. This isolates the \"Generation\" logic from the \"Retrieval\" logic."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1fbf6fdf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " TEST 1: Disease Query\n",
      "--------------------------------------------------\n",
      "To treat Citrus Canker, you can follow these steps:\n",
      "* Prune the infected twigs\n",
      "* Spray Copper Oxychloride (0.3%) at 15-day intervals\n",
      "* Ensure proper orchard hygiene\n",
      "\n",
      "Please note that when handling Copper Oxychloride, it's essential to follow safety guidelines to avoid any potential harm.\n",
      "\n",
      "Source: CitrusPlantPestsAndDiseases.pdf\n",
      "\n",
      "\n",
      " TEST 2: Scheme Query\n",
      "--------------------------------------------------\n",
      "Yes, there is a subsidy for drip irrigation. The PMKSY scheme provides up to 55% subsidy for small farmers installing Drip Irrigation. \n",
      "\n",
      "To avail of this subsidy, you will need to:\n",
      "* Submit your application via the state portal\n",
      "\n",
      "Source: GovernmentSchemes.pdf\n"
     ]
    }
   ],
   "source": [
    "# 1. Mock Context (Simulating what Phase 5 would return)\n",
    "mock_docs_disease = [\n",
    "    {\n",
    "        \"content\": \"To control Citrus Canker, prune the infected twigs. Spray Copper Oxychloride (0.3%) at 15-day intervals. Ensure proper orchard hygiene.\",\n",
    "        \"metadata\": {\"source\": \"CitrusPlantPestsAndDiseases.pdf\"}\n",
    "    },\n",
    "    {\n",
    "        \"content\": \"Citrus Canker is a bacterial disease caused by Xanthomonas. It causes corky lesions on fruit and leaves.\",\n",
    "        \"metadata\": {\"source\": \"CitrusPlantPestsAndDiseases.pdf\"}\n",
    "    }\n",
    "]\n",
    "\n",
    "mock_docs_scheme = [\n",
    "    {\n",
    "        \"content\": \"The PMKSY scheme provides up to 55% subsidy for small farmers installing Drip Irrigation. The application must be submitted via the state portal.\",\n",
    "        \"metadata\": {\"source\": \"GovernmentSchemes.pdf\"}\n",
    "    }\n",
    "]\n",
    "\n",
    "print(\" TEST 1: Disease Query\")\n",
    "print(\"-\" * 50)\n",
    "ans1 = generate_answer(\"How do I treat Canker?\", mock_docs_disease)\n",
    "print(ans1)\n",
    "\n",
    "print(\"\\n\\n TEST 2: Scheme Query\")\n",
    "print(\"-\" * 50)\n",
    "ans2 = generate_answer(\"Is there subsidy for drip irrigation?\", mock_docs_scheme)\n",
    "print(ans2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ef91e327",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Phase 6 Configuration saved\n"
     ]
    }
   ],
   "source": [
    "phase6_config = {\n",
    "    \"generation_model\": phase4_config['llm_model'],\n",
    "    \"temperature\": 0.3,\n",
    "    \"persona\": \"Expert Agricultural Assistant\",\n",
    "    \"citation_style\": \"Explicit Source Listing\",\n",
    "    \"created_at\": \"2026-01-05T07:00:00.000000\"\n",
    "}\n",
    "\n",
    "with open(project_root / \"phase6_config.json\", 'w') as f:\n",
    "    json.dump(phase6_config, f, indent=2)\n",
    "\n",
    "print(\" Phase 6 Configuration saved\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1829d26b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "agri-chatbot",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
