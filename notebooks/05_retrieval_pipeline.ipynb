{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1cdd394d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Phase 5 environment loaded\n",
      "   Project root: /Users/kaushik003/Documents/projects/agri-chatbot\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "from pathlib import Path\n",
    "from dotenv import load_dotenv\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "load_dotenv()\n",
    "project_root = Path.cwd().parent if 'notebooks' in str(Path.cwd()) else Path.cwd()\n",
    "sys.path.insert(0, str(project_root))\n",
    "\n",
    "print(\" Phase 5 environment loaded\")\n",
    "print(f\"   Project root: {project_root}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "285ad143",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import json\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "from dotenv import load_dotenv\n",
    "from typing import List, Dict, Any\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "# Load environment\n",
    "load_dotenv()\n",
    "project_root = Path.cwd().parent if 'notebooks' in str(Path.cwd()) else Path.cwd()\n",
    "sys.path.insert(0, str(project_root))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "67d9431f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Libraries loaded\n"
     ]
    }
   ],
   "source": [
    "# Vector Store & Embeddings\n",
    "from pinecone import Pinecone\n",
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "\n",
    "# Keyword Search\n",
    "from rank_bm25 import BM25Okapi\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "# Re-ranking\n",
    "from sentence_transformers import CrossEncoder\n",
    "\n",
    "# Data Processing\n",
    "import pypdf\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "\n",
    "# Download NLTK data\n",
    "try:\n",
    "    nltk.data.find('tokenizers/punkt')\n",
    "except LookupError:\n",
    "    nltk.download('punkt')\n",
    "    nltk.download('punkt_tab')\n",
    "\n",
    "print(\" Libraries loaded\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b182bb7",
   "metadata": {},
   "source": [
    "### 1. Load Configurations & Initialize Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2e52529b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Loaded Configs: Disease Index='agri-chatbot-disease'\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c3ef726e673a4559aeff9a3fac4da288",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/794 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "30af7ac7326540438b8af6d0266c50d7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/90.9M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bf14f83788014cebb2c43614cc18cd78",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8d383b57216e4508b8ddf96b7ac4a030",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.txt: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2d65fc42109a4e8bb84d67dcbb018ef4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0753d9e718a14e56a4e9a23a8b848e8f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/132 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e683b9f17ba543a794520e4dccc0f6a8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " All models initialized\n"
     ]
    }
   ],
   "source": [
    "# Load Phase 1 & 2 Configs\n",
    "with open(project_root / \"phase1_config.json\", 'r') as f: phase1_config = json.load(f)\n",
    "with open(project_root / \"phase2_config.json\", 'r') as f: phase2_config = json.load(f)\n",
    "\n",
    "print(f\" Loaded Configs: Disease Index='{phase2_config['disease_index']}'\")\n",
    "\n",
    "# 1. Initialize Pinecone (Semantic Search)\n",
    "pc = Pinecone(api_key=os.getenv(\"PINECONE_API_KEY\"))\n",
    "disease_index = pc.Index(phase2_config['disease_index'])\n",
    "scheme_index = pc.Index(phase2_config['scheme_index'])\n",
    "\n",
    "# 2. Initialize Embeddings (for query encoding)\n",
    "embeddings_model = HuggingFaceEmbeddings(\n",
    "    model_name=\"sentence-transformers/all-MiniLM-L6-v2\",\n",
    "    model_kwargs={'device': 'cpu'}\n",
    ")\n",
    "\n",
    "# 3. Initialize Cross-Encoder (for Re-ranking)\n",
    "# using a lightweight but effective model\n",
    "reranker = CrossEncoder('cross-encoder/ms-marco-MiniLM-L-6-v2')\n",
    "\n",
    "print(\" All models initialized\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0609c80d",
   "metadata": {},
   "source": [
    "### 2. Build In-Memory BM25 Index (Keyword Search)\n",
    "Since BM25 requires statistical data about term frequency across the whole corpus, we need to reconstruct the chunks exactly as we did in Phase 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d1ff0426",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Reconstructing corpus for BM25...\n",
      " Reconstructed 297 disease chunks\n",
      " Reconstructed 132 scheme chunks\n"
     ]
    }
   ],
   "source": [
    "def load_and_chunk(pdf_path: Path, doc_type: str, index_name: str):\n",
    "    \"\"\"Re-create chunks to match Pinecone IDs\"\"\"\n",
    "    # 1. Load PDF\n",
    "    reader = pypdf.PdfReader(pdf_path)\n",
    "    text_data = \"\"\n",
    "    for page in reader.pages:\n",
    "        text_data += page.extract_text() + \"\\n\\n\"\n",
    "    \n",
    "    # 2. Split (Using Phase 1 Params)\n",
    "    splitter = RecursiveCharacterTextSplitter(\n",
    "        chunk_size=phase1_config['chunking']['size'],\n",
    "        chunk_overlap=phase1_config['chunking']['overlap'],\n",
    "        length_function=len\n",
    "    )\n",
    "    chunks = splitter.split_text(text_data)\n",
    "    \n",
    "    # 3. Create Document Objects with IDs matching Phase 2\n",
    "    documents = []\n",
    "    for i, content in enumerate(chunks):\n",
    "        # Phase 2 ID format: {index_name}_{i}\n",
    "        doc_id = f\"{index_name}_{i}\"\n",
    "        documents.append({\n",
    "            \"id\": doc_id,\n",
    "            \"content\": content,\n",
    "            \"metadata\": {\"source\": pdf_path.name, \"doc_type\": doc_type}\n",
    "        })\n",
    "    return documents\n",
    "\n",
    "print(\" Reconstructing corpus for BM25...\")\n",
    "DATA_DIR = project_root / \"data\"\n",
    "\n",
    "# Re-load Disease Docs\n",
    "disease_docs = load_and_chunk(\n",
    "    DATA_DIR / \"CitrusPlantPestsAndDiseases.pdf\", \n",
    "    \"disease\", \n",
    "    phase2_config['disease_index']\n",
    ")\n",
    "\n",
    "# Re-load Scheme Docs\n",
    "scheme_docs = load_and_chunk(\n",
    "    DATA_DIR / \"GovernmentSchemes.pdf\", \n",
    "    \"scheme\", \n",
    "    phase2_config['scheme_index']\n",
    ")\n",
    "\n",
    "print(f\" Reconstructed {len(disease_docs)} disease chunks\")\n",
    "print(f\" Reconstructed {len(scheme_docs)} scheme chunks\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c6848137",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Building BM25 Indices...\n",
      "BM25 Indices ready\n"
     ]
    }
   ],
   "source": [
    "def build_bm25(documents):\n",
    "    \"\"\"Tokenize corpus and build BM25 index\"\"\"\n",
    "    tokenized_corpus = [word_tokenize(doc['content'].lower()) for doc in documents]\n",
    "    bm25 = BM25Okapi(tokenized_corpus)\n",
    "    return bm25\n",
    "\n",
    "print(\" Building BM25 Indices...\")\n",
    "bm25_disease = build_bm25(disease_docs)\n",
    "bm25_scheme = build_bm25(scheme_docs)\n",
    "print(\"BM25 Indices ready\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cacaeb87",
   "metadata": {},
   "source": [
    "### 3. Define Retrieval Functions\n",
    "We need functions for Vector Search, Keyword Search, and the Fusion logic."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "db624b4e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Function `vector_search` ready\n"
     ]
    }
   ],
   "source": [
    "def vector_search(query: str, index, top_k: int = 5) -> List[Dict]:\n",
    "    \"\"\"Semantic search using Pinecone\"\"\"\n",
    "    query_emb = embeddings_model.embed_query(query)\n",
    "    \n",
    "    results = index.query(\n",
    "        vector=query_emb,\n",
    "        top_k=top_k,\n",
    "        include_metadata=True\n",
    "    )\n",
    "    \n",
    "    # Normalize structure\n",
    "    hits = []\n",
    "    for match in results.matches:\n",
    "        hits.append({\n",
    "            \"id\": match.id,\n",
    "            \"content\": match.metadata.get(\"text\", \"\"),\n",
    "            \"score\": match.score,\n",
    "            \"metadata\": match.metadata,\n",
    "            \"method\": \"semantic\"\n",
    "        })\n",
    "    return hits\n",
    "\n",
    "print(\"Function `vector_search` ready\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "16b07c77",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Function `keyword_search` ready\n"
     ]
    }
   ],
   "source": [
    "def keyword_search(query: str, bm25_index, documents: List[Dict], top_k: int = 5) -> List[Dict]:\n",
    "    \"\"\"Keyword search using BM25\"\"\"\n",
    "    tokenized_query = word_tokenize(query.lower())\n",
    "    \n",
    "    # Get scores\n",
    "    doc_scores = bm25_index.get_scores(tokenized_query)\n",
    "    \n",
    "    # Get top_k indices\n",
    "    top_indices = np.argsort(doc_scores)[::-1][:top_k]\n",
    "    \n",
    "    hits = []\n",
    "    for idx in top_indices:\n",
    "        # Ignore zero scores (no keyword match)\n",
    "        if doc_scores[idx] > 0:\n",
    "            doc = documents[idx]\n",
    "            hits.append({\n",
    "                \"id\": doc[\"id\"],\n",
    "                \"content\": doc[\"content\"],\n",
    "                \"score\": float(doc_scores[idx]), # Use raw BM25 score\n",
    "                \"metadata\": doc[\"metadata\"],\n",
    "                \"method\": \"keyword\"\n",
    "            })\n",
    "    return hits\n",
    "\n",
    "print(\"Function `keyword_search` ready\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "818c003b",
   "metadata": {},
   "source": [
    "### 4. Reciprocal Rank Fusion (RRF)\n",
    "RRF is a robust method to combine two ranked lists without worrying about the different scales of the scores (Cosine Similarity vs BM25).\n",
    "Formula: `score = 1 / (rank + k)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8ff3457f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Function `reciprocal_rank_fusion` ready\n"
     ]
    }
   ],
   "source": [
    "def reciprocal_rank_fusion(semantic_results: List[Dict], keyword_results: List[Dict], k: int = 60):\n",
    "    \"\"\"Combine results using RRF\"\"\"\n",
    "    scores = {}\n",
    "    doc_map = {} # Keep content and metadata\n",
    "    \n",
    "    # Process Semantic Results\n",
    "    for rank, hit in enumerate(semantic_results):\n",
    "        doc_id = hit['id']\n",
    "        if doc_id not in doc_map:\n",
    "            doc_map[doc_id] = hit\n",
    "        scores[doc_id] = scores.get(doc_id, 0) + (1 / (rank + k))\n",
    "        \n",
    "    # Process Keyword Results\n",
    "    for rank, hit in enumerate(keyword_results):\n",
    "        doc_id = hit['id']\n",
    "        if doc_id not in doc_map:\n",
    "            doc_map[doc_id] = hit\n",
    "        # Add to existing score or start new\n",
    "        scores[doc_id] = scores.get(doc_id, 0) + (1 / (rank + k))\n",
    "    \n",
    "    # Sort by fused score\n",
    "    sorted_ids = sorted(scores.items(), key=lambda x: x[1], reverse=True)\n",
    "    \n",
    "    # Reconstruct final list\n",
    "    fused_results = []\n",
    "    for doc_id, score in sorted_ids:\n",
    "        doc = doc_map[doc_id]\n",
    "        doc['score'] = score\n",
    "        doc['method'] = 'hybrid'\n",
    "        fused_results.append(doc)\n",
    "        \n",
    "    return fused_results\n",
    "\n",
    "print(\"Function `reciprocal_rank_fusion` ready\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c88fedf8",
   "metadata": {},
   "source": [
    "### 5. Re-ranking with Cross-Encoder\n",
    "Semantic search and BM25 are \"Retrievers\". They are fast but not always accurate at deep understanding. \n",
    "A Cross-Encoder is a \"Re-ranker\". It takes (Query, Document) pairs and outputs a precise similarity score (0 to 1). It is slower, so we only use it on the top ~10 results from fusion."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "56d2b7f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Function `rerank_results` ready\n"
     ]
    }
   ],
   "source": [
    "def rerank_results(query: str, results: List[Dict], top_k: int = 5) -> List[Dict]:\n",
    "    \"\"\"Re-rank documents using Cross-Encoder\"\"\"\n",
    "    if not results:\n",
    "        return []\n",
    "    \n",
    "    # Prepare pairs for model\n",
    "    pairs = [[query, doc['content']] for doc in results]\n",
    "    \n",
    "    # Predict scores\n",
    "    scores = reranker.predict(pairs)\n",
    "    \n",
    "    # Attach new scores and sort\n",
    "    for i, doc in enumerate(results):\n",
    "        doc['rerank_score'] = float(scores[i])\n",
    "        \n",
    "    # Sort by re-rank score\n",
    "    reranked = sorted(results, key=lambda x: x['rerank_score'], reverse=True)\n",
    "    \n",
    "    return reranked[:top_k]\n",
    "\n",
    "print(\"Function `rerank_results` ready\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bb92561",
   "metadata": {},
   "source": [
    "### 6. Unified Retrieval Logic\n",
    "Now we combine everything into a single function that handles the logic:\n",
    "1. Identify which index to query (Disease/Scheme/Hybrid)\n",
    "2. Retrieve Semantic + Keyword\n",
    "3. Fuse\n",
    "4. Re-rank"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "68a37a66",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Advanced Retrieval Pipeline Ready!\n"
     ]
    }
   ],
   "source": [
    "def get_relevant_documents(query: str, intent: str, top_k: int = 5) -> List[Dict]:\n",
    "    \"\"\"\n",
    "    Full pipeline: \n",
    "    Routing -> Semantic+Keyword Search -> Fusion -> Re-ranking\n",
    "    \"\"\"\n",
    "    final_results = []\n",
    "    \n",
    "    # 1. Routing Logic\n",
    "    search_disease = intent in [\"disease\", \"hybrid\", \"unclear\"]\n",
    "    search_scheme = intent in [\"scheme\", \"hybrid\", \"unclear\"]\n",
    "    \n",
    "    retrieved_docs = []\n",
    "    \n",
    "    # 2a. Disease Search\n",
    "    if search_disease:\n",
    "        # Semantic\n",
    "        sem_docs = vector_search(query, disease_index, top_k=10)\n",
    "        # Keyword\n",
    "        kw_docs = keyword_search(query, bm25_disease, disease_docs, top_k=10)\n",
    "        # Fuse\n",
    "        retrieved_docs.extend(reciprocal_rank_fusion(sem_docs, kw_docs))\n",
    "        \n",
    "    # 2b. Scheme Search\n",
    "    if search_scheme:\n",
    "        # Semantic\n",
    "        sem_docs = vector_search(query, scheme_index, top_k=10)\n",
    "        # Keyword\n",
    "        kw_docs = keyword_search(query, bm25_scheme, scheme_docs, top_k=10)\n",
    "        # Fuse\n",
    "        retrieved_docs.extend(reciprocal_rank_fusion(sem_docs, kw_docs))\n",
    "    \n",
    "    # 3. Deduplicate (if hybrid fetched same doc via different paths - unlikely but safe)\n",
    "    unique_docs = {doc['id']: doc for doc in retrieved_docs}.values()\n",
    "    \n",
    "    # 4. Re-rank\n",
    "    final_results = rerank_results(query, list(unique_docs), top_k=top_k)\n",
    "    \n",
    "    return final_results\n",
    "\n",
    "print(\"Advanced Retrieval Pipeline Ready!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba3a2edc",
   "metadata": {},
   "source": [
    "### 7. Testing & Evaluation\n",
    "Let's test with queries that require specific keywords (chemicals) and conceptual understanding (subsidies)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e1fafe17",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running Tests...\n",
      "\n",
      "❓ Query: 'How to use Imidacloprid for citrus?' (Intent: disease)\n",
      "------------------------------------------------------------\n",
      "1. Score: 4.8238 | Source: CitrusPlantPestsAndDiseases.pdf\n",
      "   Excerpt: introductions: Quarantine and inspect new citrus plants for scales to avoid introducing a new  species or population.  Chemical Control: - Systemic in...\n",
      "2. Score: 3.4653 | Source: CitrusPlantPestsAndDiseases.pdf\n",
      "   Excerpt: larvae are protected inside leaves, one must either kill the adults or use a systemic that gets inside  leaf. - Systemic insecticides (neonicotinoids)...\n",
      "3. Score: 3.4653 | Source: CitrusPlantPestsAndDiseases.pdf\n",
      "   Excerpt: larvae are protected inside leaves, one must either kill the adults or use a systemic that gets inside  leaf. - Systemic insecticides (neonicotinoids)...\n",
      "\n",
      "\n",
      "❓ Query: 'Is there money help for water systems?' (Intent: scheme)\n",
      "------------------------------------------------------------\n",
      "1. Score: -9.5027 | Source: GovernmentSchemes.pdf\n",
      "   Excerpt: 50% shade net density required Area: 700 sq.m per hectare of nursery Cost: Rs. 4-5 lakh Protects saplings from excess heat/wind 3. Drip Irrigation Sys...\n",
      "2. Score: -9.6837 | Source: GovernmentSchemes.pdf\n",
      "   Excerpt: No subsidy recovery demanded (acts of nature) Shift to less water-demanding crop or rejuvenate with de\u0000cit irrigation Install water harvesting structu...\n",
      "3. Score: -10.0806 | Source: GovernmentSchemes.pdf\n",
      "   Excerpt: Farmer Category Farm Size Limit Drip Irrigation Subsidy Sprinkle r Subsidy Max. Assistance SC/ST 12.5 acres max 55-100% 50-90% Rs. 5,61,185 (HIGHEST I...\n",
      "\n",
      "\n",
      "❓ Query: 'Subsidies for treating root rot' (Intent: hybrid)\n",
      "------------------------------------------------------------\n",
      "1. Score: -3.5679 | Source: CitrusPlantPestsAndDiseases.pdf\n",
      "   Excerpt: formulations in the root zone; these antagonists can reduce pathogen buildup. - Water  management is key: if one tree got dry root rot due to under-wa...\n",
      "2. Score: -3.6323 | Source: CitrusPlantPestsAndDiseases.pdf\n",
      "   Excerpt: When to Seek Expert Help: If multiple trees start showing symptoms of root rot (especially  after drought or in a localized patch of the orchard), it’...\n",
      "3. Score: -3.6897 | Source: CitrusPlantPestsAndDiseases.pdf\n",
      "   Excerpt: also promotes beneficial microbes that outcompete pathogens. - Soil health: Incorporate organic  matter to improve soil structure and water retention....\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "test_queries = [\n",
    "    # Specific Chemical (Keyword heavy)\n",
    "    {\"query\": \"How to use Imidacloprid for citrus?\", \"intent\": \"disease\"},\n",
    "    \n",
    "    # Conceptual (Semantic heavy)\n",
    "    {\"query\": \"Is there money help for water systems?\", \"intent\": \"scheme\"},\n",
    "    \n",
    "    # Hybrid\n",
    "    {\"query\": \"Subsidies for treating root rot\", \"intent\": \"hybrid\"}\n",
    "]\n",
    "\n",
    "print(\"Running Tests...\\n\")\n",
    "\n",
    "for item in test_queries:\n",
    "    q = item['query']\n",
    "    intent = item['intent']\n",
    "    \n",
    "    print(f\"❓ Query: '{q}' (Intent: {intent})\")\n",
    "    print(\"-\" * 60)\n",
    "    \n",
    "    results = get_relevant_documents(q, intent, top_k=3)\n",
    "    \n",
    "    for i, res in enumerate(results, 1):\n",
    "        # Snippet for display\n",
    "        snippet = res['content'][:150].replace('\\n', ' ')\n",
    "        print(f\"{i}. Score: {res['rerank_score']:.4f} | Source: {res['metadata'].get('source', 'Unknown')}\")\n",
    "        print(f\"   Excerpt: {snippet}...\")\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9e0241d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Phase 5 Configuration saved\n"
     ]
    }
   ],
   "source": [
    "phase5_config = {\n",
    "    \"retrieval_strategy\": \"Hybrid (RRF) + Re-ranking\",\n",
    "    \"bm25_enabled\": True,\n",
    "    \"cross_encoder\": \"cross-encoder/ms-marco-MiniLM-L-6-v2\",\n",
    "    \"embedding_model\": \"sentence-transformers/all-MiniLM-L6-v2\",\n",
    "    \"rrf_k\": 60,\n",
    "    \"created_at\": \"2026-01-05T06:00:00.000000\"\n",
    "}\n",
    "\n",
    "config_path = project_root / \"phase5_config.json\"\n",
    "with open(config_path, 'w') as f:\n",
    "    json.dump(phase5_config, f, indent=2)\n",
    "\n",
    "print(\"Phase 5 Configuration saved\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bf98730",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "agri-chatbot",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
